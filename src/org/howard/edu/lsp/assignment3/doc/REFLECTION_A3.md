Looking back at my Assignment 2 ETL pipeline, the main difference is how the code is structured. Assignment 2 was procedural: the main method handled everything—reading the CSV, parsing fields, transforming data, writing output, and counting skipped rows. It worked, but the logic was all in one place, which made it harder to test, maintain, or reason about.
For Assignment 3, I reorganized the pipeline around objects that represent the key concepts and responsibilities. Product is now an immutable domain model with “with” methods to create updated copies. PriceRange handles classification based on price. ProductCsvCodec manages all CSV input and output, including parsing, validation, and writing. ProductTransformer applies the business rules: uppercasing names, applying the electronics discount, reclassifying premium electronics, and calculating the final price range. Finally, ETLPipeline coordinates everything and returns a Result object with statistics on how many rows were read, transformed, or skipped.
This design makes the system easier to understand. Each class has a clear purpose, responsibilities aren’t mixed together, and the public interface is minimal. I also reduced the number of classes by combining CSV parsing and writing, keeping the code readable while maintaining good object-oriented principles.

One improvement I focused on was handling edge cases more cleanly. In Assignment 2, the output header depended on the input file and could cause issues when the file was empty. Now, the output header is always correct and written first. The pipeline only processes non-header rows, missing or empty files are handled gracefully, and skipped rows are counted in the Result object.

I tested Assignment 3 carefully to make sure it behaves the same as Assignment 2. I ran the robust sample input to check that names are uppercased, electronics get the 10% discount, premium electronics are reclassified correctly, and price ranges use the final rounded price. I also tested edge cases: empty files, files with only headers, missing files, and invalid rows that should be skipped. I compared both the output CSV and the run summary to the results from Assignment 2 to confirm consistency.
Overall, Assignment 3 produces the same results as Assignment 2, but the code is more organized, easier to maintain, and easier to extend. The object-oriented approach helps keep responsibilities clear, data is encapsulated, and the design avoids large, hard-to-manage methods. This version feels more structured and aligned with the design principles we have covered.
